{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253cdb08-1e61-4a1b-b24d-989533b685a4",
   "metadata": {},
   "source": [
    "# Grammar and Online Product Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86fa1f-a93f-4237-a653-7390374b92e4",
   "metadata": {},
   "source": [
    "#### -------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1a230c-180b-4afe-af7b-87208a941b81",
   "metadata": {},
   "source": [
    "#### This is a list of over 71,045 reviews from 1,000 different products provided by Datafiniti's Product Database. The dataset includes the text and title of the review, the name and manufacturer of the product, reviewer metadata, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f079d602-d005-40b6-a59d-26c995af26aa",
   "metadata": {},
   "source": [
    "#### -------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f17d0ec-fec0-4005-98e1-adc85d97f917",
   "metadata": {},
   "source": [
    "#### Columns:\n",
    "##### 1. id\n",
    "##### 2. brand\n",
    "##### 3. categories\n",
    "##### 4. dateAdded\n",
    "##### 5. dateUpdated\n",
    "##### 6. ean\n",
    "##### 7. keys\n",
    "##### 8. manufacturer\n",
    "##### 9. manufacturerNumber\n",
    "##### 10. name\n",
    "##### 11. reviews.date\n",
    "##### 12. reviews.dateAdded\n",
    "##### 13. reviews.dateSeen\n",
    "##### 14. reviews.didPurchase\n",
    "##### 15. reviews.doRecommend\n",
    "##### 16. reviews.id\n",
    "##### 17. reviews.numHelpful\n",
    "##### 18. reviews.rating\n",
    "##### 19. reviews.sourceURLs\n",
    "##### 20. reviews.text\n",
    "##### 21. reviews.title\n",
    "##### 22. reviews.userCity\n",
    "##### 23. reviews.userProvince\n",
    "##### 24. reviews.username\n",
    "##### 25. upc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de39c60-3631-426a-8f6e-dfe5bdc1f893",
   "metadata": {},
   "source": [
    "#### -------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1277bf3c-6b06-4573-bea2-354d4e9a3b4c",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5125fb41-f6c8-4923-9f73-8e9d20f04a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import datetime as dt\n",
    "## from keras.preprocessing.text import Tokenizer\n",
    "## from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "## from keras.utils import np_utils\n",
    "## from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from numpy import loadtxt\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ff986-98c8-4b59-84db-6d6fe0d0f5ed",
   "metadata": {},
   "source": [
    "#### -------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73109a8-44e5-484d-a166-1cabb516b4b5",
   "metadata": {},
   "source": [
    "#### Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6359fc86-fa1c-48cb-bd33-c9cce66e88c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = pd.read_csv('GrammarandProductReviews.csv')\n",
    "nlp_dataset = original_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd16b677-e774-43ca-bf51-38504b98cb40",
   "metadata": {},
   "source": [
    "#### -------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da6ece-c7f6-41e6-a186-06f166b5c694",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing based on NLP Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "77dd763d-08e8-4016-9275-6944cd6b1302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71044, 25)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f070ed9b-8209-40dc-a842-a55ec84e2a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                          0\n",
       "brand                       0\n",
       "categories                  0\n",
       "dateAdded                   0\n",
       "dateUpdated                 0\n",
       "ean                     31979\n",
       "keys                        0\n",
       "manufacturer              141\n",
       "manufacturerNumber        203\n",
       "name                        0\n",
       "reviews.date               67\n",
       "reviews.dateAdded           0\n",
       "reviews.dateSeen            0\n",
       "reviews.didPurchase     38886\n",
       "reviews.doRecommend     10615\n",
       "reviews.id              38886\n",
       "reviews.numHelpful      38536\n",
       "reviews.rating              0\n",
       "reviews.sourceURLs          0\n",
       "reviews.text               36\n",
       "reviews.title             476\n",
       "reviews.userCity        65634\n",
       "reviews.userProvince    70595\n",
       "reviews.username           96\n",
       "upc                         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff461f-9c10-4897-aa9b-a7057be251bb",
   "metadata": {},
   "source": [
    "#### -------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd645db1-9bf4-4a2d-899c-315d14f529fe",
   "metadata": {},
   "source": [
    "#### Filtering columns `reviews.didPurchase` and `reviews.text` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e4872fa1-5549-4907-acc1-13bef6e18af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_dataset.dropna(subset = ['reviews.text'], inplace=True) ## 36 null values\n",
    "## nlp_dataset.dropna(subset = ['reviews.didPurchase'], inplace=True) \n",
    "nlp_dataset.dropna(subset = ['reviews.doRecommend'], inplace=True) ## 10615 null values \n",
    "nlp_dataset.dropna(subset = ['reviews.title'], inplace=True) ## 476 null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da16f04c-5ba4-491a-ba90-34542d43e16b",
   "metadata": {},
   "source": [
    "#### Dimension of the dataset after filtering, removing the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0d04bcd0-2865-4ae1-88e2-8c1096b3dcc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60340, 25)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2fbb14d0-4d41-4b97-9941-ef75a035ba27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33159"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_dataset['reviews.didPurchase'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "feb2a94c-1a13-4c66-a01a-200184c0c73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_dataset['reviews.text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2a26156d-1a19-4e69-ab77-df3a06074859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_dataset['reviews.doRecommend'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a57f421c-0cd4-4289-a1c8-49f9770db515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_dataset['reviews.title'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "39d40af7-0d88-418a-bac3-4664d5193bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    60340\n",
       "Name: reviews.doRecommend, dtype: int64"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## (nlp_dataset['reviews.didPurchase'] == nlp_dataset['reviews.doRecommend']).where(nlp_dataset['reviews.didPurchase'].notnull()).value_counts()\n",
    "nlp_dataset['reviews.doRecommend'].notnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "dd460abb-9dee-4dbc-a85f-184928d73e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_dataset['reviews.didPurchase'].fillna(nlp_dataset['reviews.doRecommend'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f5ba303c-ffee-4d79-8da0-48c9556b7536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_dataset['reviews.didPurchase'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "6ae66cc3-117e-47eb-a0e6-54acf8d8e6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_dataset.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "213a08d9-bb26-4461-bdf5-e702709ac192",
   "metadata": {},
   "outputs": [],
   "source": [
    "##nlp_dataset = nlp_dataset.astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "bdd793d6-6859-4821-98b9-ae29c0f5f547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                       object\n",
       "brand                    object\n",
       "categories               object\n",
       "dateAdded                object\n",
       "dateUpdated              object\n",
       "ean                      object\n",
       "keys                     object\n",
       "manufacturer             object\n",
       "manufacturerNumber       object\n",
       "name                     object\n",
       "reviews.date             object\n",
       "reviews.dateAdded        object\n",
       "reviews.dateSeen         object\n",
       "reviews.didPurchase        bool\n",
       "reviews.doRecommend      object\n",
       "reviews.id              float64\n",
       "reviews.numHelpful      float64\n",
       "reviews.rating            int64\n",
       "reviews.sourceURLs       object\n",
       "reviews.text             object\n",
       "reviews.title            object\n",
       "reviews.userCity         object\n",
       "reviews.userProvince     object\n",
       "reviews.username         object\n",
       "upc                      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "bd298453-b6eb-42b1-b538-e06987ad886b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 60340 entries, 3 to 71043\n",
      "Data columns (total 25 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   id                    60340 non-null  object \n",
      " 1   brand                 60340 non-null  object \n",
      " 2   categories            60340 non-null  object \n",
      " 3   dateAdded             60340 non-null  object \n",
      " 4   dateUpdated           60340 non-null  object \n",
      " 5   ean                   30576 non-null  object \n",
      " 6   keys                  60340 non-null  object \n",
      " 7   manufacturer          60199 non-null  object \n",
      " 8   manufacturerNumber    60162 non-null  object \n",
      " 9   name                  60340 non-null  object \n",
      " 10  reviews.date          60340 non-null  object \n",
      " 11  reviews.dateAdded     60340 non-null  object \n",
      " 12  reviews.dateSeen      60340 non-null  object \n",
      " 13  reviews.didPurchase   60340 non-null  bool   \n",
      " 14  reviews.doRecommend   60340 non-null  object \n",
      " 15  reviews.id            27181 non-null  float64\n",
      " 16  reviews.numHelpful    32384 non-null  float64\n",
      " 17  reviews.rating        60340 non-null  int64  \n",
      " 18  reviews.sourceURLs    60340 non-null  object \n",
      " 19  reviews.text          60340 non-null  object \n",
      " 20  reviews.title         60340 non-null  object \n",
      " 21  reviews.userCity      3212 non-null   object \n",
      " 22  reviews.userProvince  301 non-null    object \n",
      " 23  reviews.username      60287 non-null  object \n",
      " 24  upc                   60340 non-null  object \n",
      "dtypes: bool(1), float64(2), int64(1), object(21)\n",
      "memory usage: 11.6+ MB\n"
     ]
    }
   ],
   "source": [
    "nlp_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c99346-be15-4f2b-b9cb-f5a08a5c0e8e",
   "metadata": {},
   "source": [
    "#### Input and output variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "98f79055-b3c7-4b97-b30a-12ce802a2c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = nlp_dataset['reviews.text']\n",
    "y = nlp_dataset['reviews.didPurchase']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55404635-b310-4c74-bb8d-8ebd5b2e68e4",
   "metadata": {},
   "source": [
    "#### Number of purchased and not purchased products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c7ead892-83f4-4ba7-9a71-a08fdaafd778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>35149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>25191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Review Purchased\n",
       "True              35149\n",
       "False             25191"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts().to_frame().rename(columns = {'reviews.didPurchase' : 'Review Purchased'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842dd1c5-d71a-47ed-b10f-3bf35597cdc2",
   "metadata": {},
   "source": [
    "#### Removing columns `id`, `ean`, `keys`, `manufacturer`, `manufacturerNumber`, `reviews.id`, `reviews.sourceURLs`, `upc`, `reviews.UserCity`, `reviews.userProvince`, `reviews.username`, `reviews.numHelpful`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2d18d30c-39ac-44f0-acda-a98aff06d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_dataset.drop(['id', 'categories', 'ean', 'keys', 'manufacturer', 'manufacturerNumber', 'reviews.id', 'reviews.sourceURLs', 'upc', 'reviews.userCity', 'dateUpdated', 'reviews.dateSeen', 'reviews.userProvince', 'reviews.username', 'reviews.numHelpful'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8899815e-bd58-4dcd-ac1c-c2eea6a42aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60340, 10)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "31c1f17d-9bce-4340-b67c-5c4dbb4576ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brand                  0\n",
       "dateAdded              0\n",
       "name                   0\n",
       "reviews.date           0\n",
       "reviews.dateAdded      0\n",
       "reviews.didPurchase    0\n",
       "reviews.doRecommend    0\n",
       "reviews.rating         0\n",
       "reviews.text           0\n",
       "reviews.title          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ff68618d-f113-4828-92a1-35f504872dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "##nlp_dataset['Date'] = pd.to_date(nlp_dataset.dateAdded, format='%Y-%m-%d')\n",
    "nlp_dataset['dateAdded'] = pd.to_datetime(nlp_dataset['dateAdded']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "3b3d1ecc-923e-452d-b702-766df62865ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Unknown string format:  hooks slide or swivel into any desired position.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[1;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object, allow_mixed)\u001b[0m\n\u001b[0;32m   2186\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2187\u001b[1;33m             \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz_parsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime_to_datetime64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"K\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2188\u001b[0m             \u001b[1;31m# If tzaware, these values represent unix timestamps, so we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslibs\\conversion.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.conversion.datetime_to_datetime64\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unrecognized value type: <class 'str'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4948/925362130.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reviews.date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reviews.date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m    881\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtz_localize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtz\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 883\u001b[1;33m         \u001b[0mcache_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_maybe_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcache_array\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py\u001b[0m in \u001b[0;36m_maybe_cache\u001b[1;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0munique_dates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_dates\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m             \u001b[0mcache_dates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_dates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m             \u001b[0mcache_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache_dates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munique_dates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[1;31m# GH#39882 and GH#35888 in case of None and NaT we get duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minfer_datetime_format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[0mutc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtz\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"utc\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m     result, tz_parsed = objects_to_datetime64ns(\n\u001b[0m\u001b[0;32m    402\u001b[0m         \u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[0mdayfirst\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdayfirst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[1;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object, allow_mixed)\u001b[0m\n\u001b[0;32m   2191\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"i8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz_parsed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2193\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2195\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtz_parsed\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[1;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object, allow_mixed)\u001b[0m\n\u001b[0;32m   2173\u001b[0m     \u001b[0morder\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"F\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"C\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"F\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"C\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2174\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2175\u001b[1;33m         result, tz_parsed = tslib.array_to_datetime(\n\u001b[0m\u001b[0;32m   2176\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"K\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2177\u001b[0m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib._array_to_datetime_object\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib._array_to_datetime_object\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslibs\\parsing.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[0;32m   1366\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparserinfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDEFAULTPARSER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mParserError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown string format: %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimestr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Unknown string format:  hooks slide or swivel into any desired position.\""
     ]
    }
   ],
   "source": [
    "nlp_dataset['reviews.date'] = pd.to_datetime(nlp_dataset['reviews.date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "bed51bd7-9746-4cc1-ba89-50a4c7802b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_dataset.to_csv(\"nlp_dataset.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1d8ecd4b-8f46-4434-a978-1ee0c651e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []  ## <class 'list'>\n",
    "for sen in reviews:\n",
    "    docs.append(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7cc8f50b-0548-4ab2-9204-8450ed909c94",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\fatba/nltk_data'\n    - 'C:\\\\Users\\\\fatba\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\fatba\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\fatba\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\fatba\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\fatba/nltk_data'\n    - 'C:\\\\Users\\\\fatba\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\fatba\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\fatba\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\fatba\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4948/2639491469.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# Perform function on our document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocs_preprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4948/2639491469.py\u001b[0m in \u001b[0;36mdocs_preprocessor\u001b[1;34m(docs)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Lemmatize all words in documents.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4948/2639491469.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Lemmatize all words in documents.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4948/2639491469.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Lemmatize all words in documents.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mword\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \"\"\"\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\fatba/nltk_data'\n    - 'C:\\\\Users\\\\fatba\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\fatba\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\fatba\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\fatba\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(line.strip() for line in open('stopwords.txt'))\n",
    "exclude = set(string.punctuation) \n",
    "def docs_preprocessor(docs):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
    "    #Remove stop words in documents\n",
    "    docs = [[token for token in doc if not token in stop_words] for doc in docs]\n",
    "    #Remove punctuations in documents\n",
    "    docs = [[token for token in doc if not token in exclude] for doc in docs]\n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 2] for doc in docs]\n",
    "    \n",
    "    # Lemmatize all words in documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "  \n",
    "    return docs\n",
    "# Perform function on our document\n",
    "documents = docs_preprocessor(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2225a7fb-56ed-45be-91dd-c4752d5940ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
